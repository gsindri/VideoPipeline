analysis:
  audio:
    sample_rate: 16000
    hop_seconds: 0.5
    smooth_seconds: 3.0
    top: 12
    min_gap_seconds: 20.0
    pre_seconds: 8.0
    post_seconds: 22.0
    skip_start_seconds: 10.0
  motion:
    sample_fps: 3.0
    scale_width: 160
    smooth_seconds: 2.5
  scenes:
    enabled: true
    threshold_z: 3.5
    min_scene_len_seconds: 1.2
    snap_window_seconds: 1.0
  highlights:
    top: 20
    min_gap_seconds: 15.0
    skip_start_seconds: 10.0
    weights:
      audio: 0.35
      motion: 0.30
      chat: 0.20
      audio_events: 0.15
    clip:
      min_seconds: 12
      max_seconds: 60
      min_pre_seconds: 2
      max_pre_seconds: 12
      min_post_seconds: 4
      max_post_seconds: 28
  speech:
    enabled: true
    # Backend selection:
    #   - auto: prefers whisper.cpp, falls back to faster-whisper
    #   - whispercpp: fast CPU (uses all cores), GPU with Vulkan build
    #   - faster_whisper: CUDA GPU support, good CPU with VAD
    backend: auto
    
    # Strict mode: if true, don't fall back to other backends
    # Useful for A/B testing performance between backends
    strict: false
    
    # Model selection - use .en variants for English-only content (faster):
    #   Standard: tiny, base, small, medium, large, large-v2, large-v3
    #   English:  tiny.en, base.en, small.en, medium.en
    #   Quantized (whisper.cpp, faster): small.en-q8_0, small-q5_1, tiny-q8_0
    model_size: small.en  # small.en is ~2x faster than small with same quality for English
    
    language: en  # Set explicitly for faster processing (null = auto-detect)
    device: cpu  # cpu or cuda (faster-whisper only)
    compute_type: int8  # int8 (CPU), float16 (GPU)
    use_gpu: false  # Request GPU acceleration if available
    
    # Performance tuning:
    threads: 0  # 0 = use all CPU cores (recommended)
    n_processors: 1  # Parallel decoders for long files (whisper.cpp, try 2-4 for >30min)
    
    # VAD (Voice Activity Detection) - CRITICAL for speed:
    #   - faster-whisper: uses built-in silero VAD
    #   - whisper.cpp: uses silero-vad if torch is installed
    #   - Can be 2-5x speedup on streams with silence/downtime
    vad_filter: true
    
    # Word timestamps: enables per-word timing for subtitles/captions
    # Disable if you only need segment timing (faster, less overhead)
    word_timestamps: true
    
    hop_seconds: 0.5
  reaction_audio:
    enabled: true
    sample_rate: 16000
    hop_seconds: 0.5
    smooth_seconds: 1.5
  audio_events:
    enabled: true
    hop_seconds: 0.5
    smooth_seconds: 2.0
    sample_rate: 16000
    events:
      laughter: 1.0
      cheering: 0.7
      applause: 0.5
      screaming: 0.8
      shouting: 0.6
  rerank:
    enabled: true
    weights:
      highlight: 0.55
      reaction: 0.25
      speech: 0.20
    hook:
      max_chars: 60
      window_seconds: 4.0
      phrases:
        - "no way"
        - "oh my god"
        - "let's go"
        - "lets go"
        - "bro"
        - "what"
        - "holy"
        - "wtf"
        - "omg"
        - "insane"
        - "clutch"
        - "no shot"
        - "bruh"
        - "dude"
        - "sheesh"
        - "yo"
        - "oh no"
        - "wait"
        - "huh"
        - "wow"
        - "nice"
        - "crazy"
        - "dead"
        - "gg"
        - "pog"
        - "hype"
    quote:
      max_chars: 120

export:
  template: vertical_streamer_pip
  width: 1080
  height: 1920
  fps: 30
  crf: 20
  preset: veryfast
  normalize_audio: false

overlay:
  hook_text:
    enabled: true
    duration_seconds: 2.0
    text: null
    font: auto
    fontsize: 64
    y: 120

layout:
  pip:
    position: top_left
    margin: 40
    width_fraction: 0.28
    border_px: 6

captions:
  enabled: false
  model_size: small
  language: null
  device: cpu
  compute_type: int8

context:
  enabled: true
  top_n: 25
  silence_noise_db: -30.0
  silence_min_duration: 0.3
  max_sentence_words: 30

  variants:
    short:
      min_s: 16
      max_s: 24
    medium:
      min_s: 24
      max_s: 40
    long:
      min_s: 40
      max_s: 75

  boundaries:
    prefer_silence: true
    prefer_sentences: true
    prefer_scene_cuts: true
    prefer_chat_valleys: true
    chat_valley_window_s: 12.0

ai:
  director:
    enabled: true
    engine: llama_cpp_server
    endpoint: "http://127.0.0.1:11435"
    model_name: "local-gguf-vulkan"
    timeout_s: 30
    max_tokens: 256
    temperature: 0.2
    platform: "shorts"
    fallback_to_rules: true
    # Auto-start server config (saves VRAM when not in use)
    server_path: "C:/llama.cpp/llama-server.exe"
    model_path: "C:/llama.cpp/models/qwen2.5-7b-instruct-q4_k_m.gguf"
    auto_start: true
    # Auto-stop server after idle (seconds). Frees VRAM for gaming.
    # Set to null to disable auto-stop. Default: 600 (10 minutes)
    auto_stop_idle_s: 600
