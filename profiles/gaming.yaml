analysis:
  audio:
    sample_rate: 16000
    hop_seconds: 0.5
    smooth_seconds: 3.0
    top: 12
    min_gap_seconds: 20.0
    pre_seconds: 8.0
    post_seconds: 22.0
    skip_start_seconds: 10.0
  # Voice Activity Detection (VAD) - primary speech boundary signal
  # More robust than silence detection for gaming/streaming content
  vad:
    enabled: true
    sample_rate: 16000
    hop_seconds: 0.5
    threshold: 0.5
    min_silence_duration_ms: 250
    speech_pad_ms: 60
    min_speech_duration_ms: 250
    use_onnx: false
    opset_version: 16
    torch_threads: 1
    device: cpu  # Use "cuda" if you have GPU
  motion:
    sample_fps: 3.0
    scale_width: 160
    smooth_seconds: 2.5
  scenes:
    enabled: true
    threshold_z: 3.5
    min_scene_len_seconds: 1.2
    snap_window_seconds: 1.0
  highlights:
    top: 20
    min_gap_seconds: 15.0
    skip_start_seconds: 10.0
    weights:
      audio: 0.35
      motion: 0.30
      chat: 0.20
      audio_events: 0.15
    clip:
      min_seconds: 12
      max_seconds: 60
      min_pre_seconds: 2
      max_pre_seconds: 12
      min_post_seconds: 4
      max_post_seconds: 28
    # LLM-powered quality filtering
    # This lets the LLM reject boring candidates even if they have high signal scores
    llm_filter_enabled: true
    llm_filter_min_quality: 5  # 1-10 scale, candidates below this are rejected
    llm_filter_max_keep: 10    # Keep at most this many candidates (null = no limit)
    content_type: gaming       # Helps LLM understand what "good content" means
    # LLM semantic scoring (reranking weight, separate from filtering)
    llm_semantic_enabled: true
    llm_semantic_weight: 0.3   # 30% LLM influence on final score
    llm_max_candidates: 15     # Max candidates to send to LLM for scoring
  speech:
    enabled: true
    # Backend selection:
    #   - openai_whisper: GPU support for AMD (ROCm) and NVIDIA (CUDA) - RECOMMENDED
    #   - faster_whisper: CUDA GPU only, fastest on NVIDIA
    #   - whispercpp: fast CPU, GPU with custom Vulkan build
    #   - auto: selects best available (prefers openai_whisper with GPU)
    backend: openai_whisper
    
    # Strict mode: if true, don't fall back to other backends
    # Useful for A/B testing performance between backends
    strict: false
    
    # Model selection - use .en variants for English-only content (faster):
    #   Standard: tiny, base, small, medium, large, large-v2, large-v3
    #   English:  tiny.en, base.en, small.en, medium.en
    #   Quantized (whisper.cpp, faster): small.en-q8_0, small-q5_1, tiny-q8_0
    model_size: small.en  # small.en is ~2x faster than small with same quality for English
    
    language: en  # Set explicitly for faster processing (null = auto-detect)
    device: cuda  # cpu or cuda
    compute_type: float16  # int8 (CPU), float16 (GPU)
    use_gpu: true  # GPU acceleration (AMD ROCm or NVIDIA CUDA)
    
    # Performance tuning:
    threads: 0  # 0 = use all CPU cores (recommended for CPU fallback)
    n_processors: 1  # Parallel decoders for long files (whisper.cpp, try 2-4 for >30min)
    
    # VAD (Voice Activity Detection) - CRITICAL for speed:
    #   - faster-whisper: uses built-in silero VAD
    #   - whisper.cpp: uses silero-vad if torch is installed
    #   - Can be 2-5x speedup on streams with silence/downtime
    vad_filter: true
    
    # Word timestamps: enables per-word timing for subtitles/captions
    # Disable if you only need segment timing (faster, less overhead)
    word_timestamps: true
    
    # Speaker diarization (who is talking):
    #   Requires pyannote-audio: pip install pyannote-audio
    #   Adds speaker labels (SPEAKER_00, SPEAKER_01, etc.) to transcript
    diarize: true  # Enable via UI checkbox or set true here
    # hf_token: Use HF_TOKEN environment variable instead (security best practice)
    
    hop_seconds: 0.5
  reaction_audio:
    enabled: true
    sample_rate: 16000
    hop_seconds: 0.5
    smooth_seconds: 1.5
  audio_events:
    enabled: true
    hop_seconds: 0.5
    smooth_seconds: 2.0
    sample_rate: 16000
    backend: pytorch  # Use PyTorch VGGish with ROCm GPU acceleration
    events:
      laughter: 1.0
      cheering: 0.7
      applause: 0.5
      screaming: 0.8
      shouting: 0.6
  # Boundary graph configuration - controls clip start/end snapping
  boundaries:
    prefer_vad: true           # Use VAD speech boundaries (primary)
    vad_boundary_score: 1.0    # Weight for VAD boundaries
    prefer_silence: true       # Also use silence as fallback
    prefer_sentences: true     # Snap to sentence boundaries
    prefer_scene_cuts: true    # Snap to visual scene cuts
    prefer_chapters: true      # Respect semantic chapter boundaries
    prefer_chat_valleys: true  # Consider chat activity lulls
    snap_tolerance_s: 1.0      # How close a boundary needs to be
    chapter_boundary_score: 2.0  # Higher weight for chapter boundaries
  # Enrichment: extract hook/quote text for candidates (score fusion is in highlights)
  enrich:
    enabled: true
    hook:
      max_chars: 60
      window_seconds: 4.0
      phrases:
        - "no way"
        - "oh my god"
        - "let's go"
        - "lets go"
        - "bro"
        - "what"
        - "holy"
        - "wtf"
        - "omg"
        - "insane"
        - "clutch"
        - "no shot"
        - "bruh"
        - "dude"
        - "sheesh"
        - "yo"
        - "oh no"
        - "wait"
        - "huh"
        - "wow"
        - "nice"
        - "crazy"
        - "dead"
        - "gg"
        - "pog"
        - "hype"
    quote:
      max_chars: 120

export:
  template: vertical_streamer_pip
  width: 1080
  height: 1920
  fps: 30
  crf: 20
  preset: medium  # Better quality encoding (adds ~10-30s per clip vs veryfast)
  normalize_audio: false

overlay:
  hook_text:
    enabled: true
    duration_seconds: 2.0
    text: null
    font: auto
    fontsize: 64
    y: 120

layout:
  pip:
    position: top_left
    margin: 40
    width_fraction: 0.28
    border_px: 6

captions:
  enabled: false
  model_size: medium.en  # Better accuracy for English, handles background noise well
  language: en
  device: cpu
  compute_type: int8

context:
  enabled: true
  top_n: 25
  silence_noise_db: -30.0
  silence_min_duration: 0.3
  max_sentence_words: 30

  variants:
    short:
      min_s: 16
      max_s: 24
    medium:
      min_s: 24
      max_s: 40
    long:
      min_s: 40
      max_s: 75

  boundaries:
    prefer_silence: true
    prefer_sentences: true
    prefer_scene_cuts: true
    prefer_chat_valleys: true
    prefer_chapters: true
    chat_valley_window_s: 12.0

  chapters:
    enabled: true
    min_chapter_len_s: 60.0
    max_chapter_len_s: 900.0
    embedding_model: "all-mpnet-base-v2"  # Higher quality embeddings for accurate topic detection
    changepoint_method: "pelt"
    changepoint_penalty: 10.0
    snap_to_silence_window_s: 10.0
    llm_labeling: true
    max_chars_per_chapter: 6000

ai:
  director:
    enabled: true
    engine: llama_cpp_server
    endpoint: "http://127.0.0.1:11435"
    model_name: "local-gguf-vulkan"
    timeout_s: 60
    max_tokens: 2048  # Increased for highlight scoring (15 candidates Ã— ~100 tokens each)
    temperature: 0.2
    platform: "shorts"
    fallback_to_rules: true
    # Auto-start server config (saves VRAM when not in use)
    server_path: "C:/llama.cpp/llama-server.exe"
    model_path: "C:/llama.cpp/models/qwen2.5-7b-instruct-q4_k_m.gguf"
    auto_start: true
    # Startup timeout for LLM server (seconds). Large models (7B+) need 60-120s to load.
    # Increase this if you see "LLM not available" errors during download.
    startup_timeout_s: 120
    # Auto-stop server after idle (seconds). Frees VRAM for gaming.
    # Set to null to disable auto-stop. Default: 600 (10 minutes)
    auto_stop_idle_s: 600
